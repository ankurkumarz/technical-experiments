[{"id":0,"href":"/docs/dataengineering/dbt/","title":"Dbt","section":"Data Engineering","content":" dbt (Data Build Tool) # Context # dbt is a SQL-first transformation workflow that lets teams quickly and collaboratively deploy analytics code following software engineering best practices like modularity, portability, CI/CD, and documentation. dbt uses Jinja as the templating engine to generate SQL code for data transformations such as {{ ref() }} function: select * from {{ref(\u0026#39;model_a\u0026#39;)}} Jinja Tutorial dbt Setup in Cloud # dbt Cloud is a hosted service for managing your dbt projects and deployments launched in 2019. Click here to know more details. dbt Reference Login credentials: @hotmail.com / dbt Setup on Local # Install dbt project Click here to know more: brew update brew tap dbt-labs/dbt brew install dbt-postgres conda create -e dbt pip install dbt-postgres Create a dbt project: dbt init dbt_sandbox dbt build dbt run Different between dbt build \u0026amp; run: dbt run = execute your models. dbt build = execute your models and test them. dbt - Getting started Guides # dbt Guides dbt Fundamentals Course dbt with BiqQuery Jinja Examples # Complete Examples\nFor loop:\n{% set payment_methods = [\u0026#34;bank_transfer\u0026#34;, \u0026#34;credit_card\u0026#34;, \u0026#34;gift_card\u0026#34;] %} select order_id, {% for payment_method in payment_methods %} sum(case when payment_method = \u0026#39;{{payment_method}}\u0026#39; then amount end) as {{payment_method}}_amount, {% endfor %} sum(amount) as total_amount from {{ ref(\u0026#39;raw_payments\u0026#39;) }} group by 1 Writing modular macros: {% macro get_column_values(column_name, relation) %} {% set relation_query %} select distinct {{ column_name }} from {{ relation }} order by 1 {% endset %} {% set results = run_query(relation_query) %} {% if execute %} {# Return the first column #} {% set results_list = results.columns[0].values() %} {% else %} {% set results_list = [] %} {% endif %} {{ return(results_list) }} {% endmacro %} {% macro get_payment_methods() %} {{ return(get_column_values(\u0026#39;payment_method\u0026#39;, ref(\u0026#39;raw_payments\u0026#39;))) }} {% endmacro %} "},{"id":1,"href":"/docs/dataengineering/dremio/","title":"Dremio","section":"Data Engineering","content":" Dremio # Dremio is a data virtualization platform that allows you to connect to and query data from various sources, including databases, cloud storage, and APIs. Dremio is a good alternative to Databricks for querying data. Dremio is open source and available on GitHub. Dremio uses open source libraries for various components. Apache Arrow is used for data encoding. Apache Parquet is used for storage. Started as a query engine has evolved to support data lake capabilities. Architecture # Local Setup # Using Docker for local setup. Kubernetes for local setup using Rancher. cd sandbox/data/dremio git clone https://github.com/dremio/dremio-cloud-tools.git --depth=1 "},{"id":2,"href":"/docs/dataengineering/duckdb/","title":"Duckdb","section":"Data Engineering","content":" DuckDB # DuckDB is an open-source, in-process OLAP database used by data professionals to analyze data quickly and efficiently. Released in 2019, DuckDB is emerging as in-memory/in-process SQL-based processing engine for OLAP workload. It is Opensource and available with MIT license. DuckDB is used at Facebook, Google, and Airbnb. Enterprise support: DuckDB Labs available as a consulting firm along with MotherDuck (which has received $100m funding for its data platform based on DuckDB, with investors including Andreessen Horowitz) Supports vector database capabilities with HNSW search Processing supported: CSV, JSON, Parquet, and Apache Arrow, as well as several databases, like MySQL, SQLite, and Postgres. DuckDB in Action Book DuckDB is a modern embedded analytics database that runs on your machine and lets you efficiently process and query gigabytes of data from different sources.\nUse-cases - where to use? # Access huge volume of data (up to GBs) locally and process quickly. It is easier to set up than a Spark cluster, has lower resource requirements than pandas, easier to set up and use than PostgreSQL, Redshift, and other RDBMS, faster than SQLite, Talend for analytical workloads, and more. Data doesn’t leave your system (local or privacy-compliant hosting) RAG Solution with DuckDB Building AI Agents with DuckDB, DLT, Cognee MotherDuck extends the functionality for building AI apps using EMBED and PROMPT functions. Sample use-cases such as analyzing log files where they are stored, without needing to copy them to new locations, preprocessing and pre-cleaning of user-generated data for machine learning, etc. training Cloud vendors offer expensive analytics services,like BigQuery, Amazon Redshift, and AWS Athena, which charge by processed data volume. Replace many of those uses with scheduled cloud functions processing the data with DuckDB. You can also chain those processing functions by writing out intermediate results to cloud storage, which can then also be used for auditing. Where not to use? # Not suitable for streaming data as data needs to be available Data volumes exceed a few hundred gigabytes. The data volumes you can process with DuckDB are mostly limited by the main memory of the system. Minimal support for transactions and parallel write access Integration # Many extensions available for extending the functionality Sample Projects # DuckDB as high-performance data pipeline DuckDB in Action Book Code Cloud # MotherDuck provides hosted version of DuckDB (registered using GitHub): app.motherduck.com Integration # Local Setup # brew install duckdb duckdb .exit -- DOT commands vi $HOME/.duckdbrc -- Duck head prompt .prompt \u0026#39;O\u0026gt; \u0026#39; -- Example SQL statement select \u0026#39;Ankur | Begin quacking now \u0026#39;||cast(now() as string) as \u0026#34;Ready, Set, ...\u0026#34;; Common Commands # DESCRIBE SELECT * FROM duckdb_extensions(); INSTALL httpfs; LOAD httpfs; SELECT count(*) FROM \u0026#39;https://github.com/bnokoro/Data-Science/raw/master/\u0026#39; \u0026#39;countries%20of%20the%20world.csv\u0026#39;; .mode line SELECT * FROM read_csv_auto(\u0026#34;https://bit.ly/3KoiZR0\u0026#34;) LIMIT 1; duckdb -csv \\ -s \u0026#34;SELECT Country, Population, Birthrate, Deathrate FROM read_csv_auto(\u0026#39;https://bit.ly/3KoiZR0\u0026#39;) WHERE trim(region) = \u0026#39;WESTERN EUROPE\u0026#39;\u0026#34; \\ \u0026gt; western_europe.csv PARQUET handling # Exporting data in Parquest format duckdb \\ -s \u0026#34;COPY ( SELECT Country, Population, Birthrate, Deathrate FROM read_csv_auto(\u0026#39;https://bit.ly/3KoiZR0\u0026#39;) WHERE trim(region) = \u0026#39;WESTERN EUROPE\u0026#39; ) TO \u0026#39;western_europe.parquet\u0026#39; (FORMAT PARQUET)\u0026#34; duckdb -s \u0026#34;FROM \u0026#39;western_europe.parquet\u0026#39; LIMIT 5\u0026#34; Reading Parquet Data select * from read_parquet(\u0026#39;https://oedi-data-lake.s3.amazonaws.com/pvdaq/parquet/metrics/metrics__system_10__part000.parquet\u0026#39;) FROM parquet_schema( \u0026#39;s3://us-prd-md-duckdb-in-action/nyc-taxis/yellow_tripdata_2022-06.parquet\u0026#39;) SELECT name, type; -- 118M records CREATE OR REPLACE VIEW allRidesView AS FROM \u0026#39;s3://us-prd-md-duckdb-in-action/nyc-taxis/ yellow_tripdata_202*.parquet\u0026#39;; .timer on SUMMARIZE allRidesView; ICEBERG handling # Reference INSTALL iceberg; LOAD iceberg; SELECT count(*) FROM iceberg_scan(\u0026#39;data/iceberg/lineitem_iceberg\u0026#39;, allow_moved_paths = true); Reference Datasets # Tennis Dataset Photovoltaic Data Acquisition (PVDAQ) Public Datasets Stackoverflow Data Stack Exchange Data on Archive New York Taxi Data Motherduck UI # "},{"id":3,"href":"/docs/dataengineering/unitycatalog/","title":"Unitycatalog","section":"Data Engineering","content":" Unity Catalog # Unity Catalog is a unified data management service for Databricks. It allows you to manage your data across Databricks workspaces and Databricks SQL warehouses. Databricks open sourced Unity Catalog in June 2024. Unity Catalog on GitHub Unity Catalog Website Local Setup # Cloned from the GitHub repo bin/start-uc-server UI is separate NodeJS app and need to be started separately. Current observation is UI is very basic, not very useful. You can\u0026rsquo;t do any write operations through the UI. cd ui -- ../sandbox/data/unitycatalog/ui yarn install yarn start You can use integration with DuckDB to query Unity Catalog tables. From Duck, you can query but can\u0026rsquo;t write. Other option is to use Unity catalog\u0026rsquo;s utility CLI uc to load/query data into Unity Catalog tables: Reference: Unity Catalog CLI bin/uc table create --full_name unity.default.test --columns \u0026#34;some_numbers INT, some_letters STRING\u0026#34; --storage_location ./sandbox/data/unitycatalog/etc/data/external/unity/default/tables/ bin/uc table get --full_name unity.default.numbers "},{"id":4,"href":"/docs/datascience/metaflow/","title":"Metaflow","section":"Data Science \u0026 ML","content":" Metaflow # [Metaflow](https://metaflow.org/, built by Netflix and open-sourced in 2019. Reference Blog It is a Python library that helps developers and data scientists build, deploy, and run data-intensive applications. Metaflow provides a unified API to the whole infrastructure stack that is required to execute data science projects from prototype to production. Technologies Integration: Modeling (Python libs) Deployment (Argo Workflows, Step Functions, Apache Airflow) Versioning (Local Metadata, Metadata Service) Orchestrator (Local Orchestrator) Compute (Local Processes, AWS Batch, Kubernetes) Data (Local Datastore, AWS S3, Azure Blob Storage, Google Cloud Storage) Local Installation # cd sandbox/genai/metaflow metaflow tutorials pull metaflow tutorials list "},{"id":5,"href":"/docs/genai/llmware/","title":"Llmware","section":"Gen AI","content":" Context # LLMWare is a unified framework for building enterprise RAG pipelines with small, specialized models Examples Start Building Multi-Model Agents Locally on a Laptop Quickstart # Local Setup ../sandbox/genai/llmware pip3 install llmware "},{"id":6,"href":"/docs/lowcodeplatforms/outsystems/","title":"Outsystems","section":"Low-code Development Platform (LCDP)","content":" Outsystems # Context # Low-code development Platform\n"},{"id":7,"href":"/docs/microservices/dapr/","title":"Dapr","section":"Microservices","content":" DAPR # Dapr is a portable, event-driven runtime that makes it easy for any developer to build resilient, stateless and stateful applications that run on the cloud and edge and embraces the diversity of languages and developer frameworks. The Distributed Application Runtime (Dapr) provides APIs that simplify microservice connectivity. Dapr’s sidecar take care of the complex challenges such as service discovery, message broker integration, encryption, observability, and secret management Architecture # Sample Reference Application # Source\nLocal Installation (using Kubernetes) # Step 1: Install DAPR CLI library. Click here to read more. brew install dapr/tap/dapr-cli Setup DAPR on K8S Cluster (creates a namespace). Click here for detailed instructions for different K8S setup (AKS, EKS, GKE). dapr init --kubernetes --wait Verify Installation dapr status -k kubectl get pods --namespace dapr-system DAPR State Store \u0026amp; Pub/Sub # DAPR needs a state store (you can use Redis, CosmosDB, DynamoDB, Cassandra, etc.) Create and configure a state store with REDIS helm repo add bitnami https://charts.bitnami.com/bitnami helm repo update helm install redis bitnami/redis Create 2 files to install REDIS:redis-state.yaml, redis-pubsub.yaml (as per details here) Install in K8S quickstarts/tutorials/hello-kubernetes/deploy kubectl apply -f redis-state.yaml kubectl apply -f redis-pubsub.yaml DAPR with Docker (No K8S) # Run DAPR with docker dapr init dapr run --app-id myapp --dapr-http-port 3500 These tell Dapr to use the local Docker container for Redis as a state store and message broker (~/.dapr/components)\nRemove Dapr containers dapr uninstall DAPR UI Dashboards # Run to access DAPR UI Dashboard (-k for Kubernetes) dapr dashboard -k -n \u0026lt;your-namespace\u0026gt;. Key Takeaways # DAPR usage is more to provide developer decoupled runtime to address service-to-service communication, state management for stateful services, PUB/SUB for event-driven services, observability at fine-grained level, and related needs. For example, you can also directly use REDIS in your application code for state management but DAPR runtime provides abstraction so that you don\u0026rsquo;t have to worry about integration with state store. You will simply interface with DAPR and if there is need to change from REDIS to cloud-provider managed state such as DynamoDB, AWS DynamoDB, it can happen without any code change. More suitable for large-scale distributed applications using Microservices architecture DAPR vs. Service Mesh # DAPR is more developer centric and Service mesh is more infrastructure centric If you need encrypted communication between services, need to use Service Mesh Click here to read the official documentation References # Baeldung example with Spring Cloud Gateway (without Kubernetes DAPR Tutorials "},{"id":8,"href":"/docs/microservices/linkerd/","title":"Linkerd","section":"Microservices","content":" Linkerd # Architecture_Diagram # source: https://linkerd.io/2.11/reference/architecture/\nLocal Installation # Install Linkerd CLI: brew install linkerd linkerd version Verify K8S Cluster \u0026amp; Install: linkerd check --pre linkerd install --set proxyInit.runAsRoot=true | kubectl apply -f - kubectl get po --namespace=linkerd Check Linkerd Installation: linkerd check Installation viz extension, which provides observability and visualization linkerd viz install | kubectl apply -f - linkerd viz dashboard \u0026amp; Install Demo App in emojivoto namespace: curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSfL https://run.linkerd.io/emojivoto.yml \\ | kubectl apply -f - ## injecting linkerd proxies kubectl get -n emojivoto deploy -o yaml \\ | linkerd inject - \\ | kubectl apply -f - kubectl -n emojivoto port-forward svc/web-svc 8080:80 ## to verify linkerd -n emojivoto check --proxy check the dashboard # Link: http://localhost:50750/namespaces/emojivoto/deployments/web Key Takeaways # Installation process is very simple and \u0026ldquo;linkerd\u0026rdquo; CLI has made it very easy to install. Linkerd CLI \u0026ldquo;linked check\u0026rdquo; is very useful for any configuration mismatch and prerequisites. "},{"id":9,"href":"/docs/observability/grafana/","title":"Grafana","section":"Observability","content":" Grafana # Grafana is an opensource observability tool to query and visualize the data using interactive dashboards, etc. Grafana is one of the most popular companion solution with Prometheus, which can be queried from Grafana. Grafana supports other time-series databases like Prometheus, InfluxDB, and Graphite, monitoring platforms such as Sensu, Icinga, Checkmk,Zabbix, Netdata, and PRTG; SIEMs such as Elasticsearch and Splunk; and other data sources. GitHub Repo: https://github.com/grafana/grafana Developer: Grafana Labs, Language: Go Cloud Offering # Grafana offers managed service as a cloud offering. You can register for Free to use Grafana Cloud using GitHub or Google account credentials by clicking here. Check out Pricing here. Their Free tier offers 3 users, 14 day retention, Synthetic, and Alerting with 10k metrics + 50GB logs + 50GB traces. Pro has Grafana ML, SSO/SAML integration, Query caching, Reporting, Data source permissions, etc. Monitoring Kubernetes (Local Cluster - Grafana on Cloud) # My URL: https://ankurkumarz.grafana.net/ Refer to full documentation here. Install the Agent in Kubernetes default cluster MANIFEST_URL=https://raw.githubusercontent.com/grafana/agent/main/production/kubernetes/agent-bare.yaml NAMESPACE=default /bin/sh -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/grafana/agent/release/production/kubernetes/install-bare.sh)\u0026#34; | kubectl apply -f - Run the following commands to install kube-state-metrics: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts \u0026amp;\u0026amp; helm repo update \u0026amp;\u0026amp; helm install ksm prometheus-community/kube-state-metrics --set image.tag=v2.2.0 Configure and restart the agent (instructions in above doc link) K8S Default Dashboards # Key Findings # Grafana is a de facto standard for visualization with real-time timeseries database like Prometheus\nKey competition: Elastic\u0026rsquo;s Kibana, Cloud-providers native experience (e.g. AWS CloudWatch, Azure Monitor) or Observability/APM solutions like Dynatrace, New Relic, Splunk, etc.\nGrafana can be integrate with all the above tools to visualize data in a single place.\nGrafana key integrators:\nMonitor Azure Services using Grafana Fully-managed Grafana services are being offered by:\nAWS Fully Managed Service Grafana Labs signed partnership with Microsoft Azure to deliver first-party Grafana Service. Click here to read more. End-to-end Observability (Grafana Labs offers a managed service)\nVisualization with Grafana Metrics with Prometheus and Graphite Logs with Loki Traces with Tempo Managed Service by Grafana Cloud by Grafana Labs # "},{"id":10,"href":"/docs/observability/prometheus/","title":"Prometheus","section":"Observability","content":" Prometheus # Prometheus is an open-source systems monitoring and alerting toolkit originally built at SoundCloud (2021). Prometheus collects and stores its metrics as time series data Prometheus was the second project accepted into the Cloud Native Computing Foundation after Kubernetes, and also the second to graduate. Prometheus and Grafana can be used as complementary services that, together, provide a robust time-series database with excellent data visualization. Primarily relies on Polling to pull data from the application or any system. Client libraries are available for instrumenting the applications in many supported languages like Java, Python, Go, etc. Main Features (Official Docs) # Click here for official docs.\nOfffers a multi-dimensional data model with time series data Supports a Query Language - PromQL No reliance on distributed storage; single server nodes are autonomous Time series collection happens via a pull model over HTTP Pushing time series is supported via an intermediary gateway Targets are discovered via service discovery or static configuration Multiple modes of graphing and dashboarding support- Architecture # Alternative Options # InfluxDB, Graphite for Time series data ELK (Elastic, Logstash, Kibana) Cloud Providers - AWS CloudWatch, Azure Monitor Observability/APM Vendors - New Relic, Dynatrace, etc. Local Installation # Any of the below options:\nDownload the JAR and Run directly Use brew install brew install prometheus #Run directly with /usr/local/opt/prometheus/bin/prometheus_brew_services #Or automatic installation brew services restart prometheus Docker with default configuration docker run -p 9090:9090 prom/prometheus Local access # URL: http://localhost:9090/, http://localhost:9090/metrics Enter Expression: prometheus_target_interval_length_seconds or rate(prometheus_tsdb_head_chunks_created_total[1m]) Read more about getting started here.\nManaged Services # AWS Managed Service for Prometheus Grafana Labs Prometheus "},{"id":11,"href":"/docs/programming/rust-webassembly/","title":"Rust Webassembly","section":"Programming","content":" Rust and WebAssembly support # Rust is the emerging programming language and this experimentation is to use Rust code as WebAssembly package to be executed in the browser (e.g. Chrome) For example - JavaScript calling the Rust function directly into the browser or vice-versa. Click here to start learning Rust. Rust book is also available for free. Click here to know more about WebAssembly (WASM) and Rust example with WASM. Installation and project generation # All instructions are for MacOS only. Install Rust using brew and check Cargo (Rust\u0026rsquo;s package manager): Rust is installed and managed by the rustup tool. Click here to learn more. brew install rustup rustup-init cargo --version Generate a Rust WebAssembly project cargo new --lib rust-wasm cargo build Build the WASM package (add ~/.cargo/bin to your PATH) wasm-pack build --target web Run the Web Server in root folder: python3 -m http.server It is running on: http://localhost:8000 "},{"id":12,"href":"/docs/programming/rust-webassembly/code/rust-wasm/","title":"Index","section":"Code","content":"\u003c!DOCTYPE html\u003e hello-wasm example Web Assembly with Rust "},{"id":13,"href":"/docs/workflow/camunda/","title":"Camunda","section":"Workflow","content":" Camunda # Camunda describes itself as a universal process orchestrator. The most recent version is Camunda Platform 8. A workflow management solution based on BPMN and DMN based modeling framework. It supports both self-managed PaaS-based deployment and SaaS-based deployment model. Underlying architecture is using Microservices \u0026amp; Cloud-native oriented architecture replacing traditional Traditional business process management suites (BPM) systems. Camunda has Opensource Modeler (as a desktop app \u0026amp; web), Opensource community edition. See comparison with enterprise edition by clicking here. Key offerings of Camunda Platform 8 are documented here Horizontal scalability, high availability, and fault tolerance (with SaaS Model \u0026amp; On-premises/Hybrid Cloud as a separate option) Audit trail of workflows (events-driven) Reactive publish-subscribe interaction model, Visual processes modeled in ISO-standard BPMN 2.0, and Language-agnostic client model. Applicability # Business-process automation examples such as: Financial Services: Automated KYC process for client onboarding (Bank), client engagement process automation Any human-task orchestration Orchestrating Microservices RPA Bots Orchestration Build a Centralized Process Automation Platform. Read here. Architecture # Overall Technology Stack: Java, Spring Boot, Kafka, gRPC (Zeebee components communication), GraphQL (used by Tasklist), ELK (logs for Zeebee and others) Camunda 7 was Java EE \u0026amp; Spring Boot based and Camunda 8 has been reengineered towards cloud-native and microservices. GitHub Repository: https://github.com/camunda Platform Components: Zeebe, Modeler, Operate, Tasklist \u0026amp; Optimize. Modeler: Modeling following BPMN/DNM standards visually (web \u0026amp; desktop app) Zeebee as a cloud-native BPMN Workflow Engine. Decision Engine: using DNM as standard. Tasklist: for managing human tasks. Tasklist is a ready-to-use application to rapidly implement business processes alongside user tasks in Zeebe. Operate: for monitoring real-time, analyze/resolve issues. Operate is a tool for monitoring and troubleshooting process instances running in Zeebe. Optimize: for insights into the workflow with visual reports, heatmap, etc. Image Source: https://camunda.com/platform-7/\nZeebee Architecture: Read about Camunda platform reference architecture by clicking here:\nCamunda Process Engine is built using Spring Boot. It supports embedded deployment (as a library) or container-managed as an independent container, or standalone process engine server deployed in VM. Docker image is available at their registry built using Harbor.\nCamunda recommends Oracle or PostgreSQL for production and H2 for development. It supports MySQL, MariaDB, IBM DB2, Amazon Aurora, Azure SQL, SQL Server, and CockroachDB as well\nAccess/Setup # Trial: SaaS is available as a free account with collaborative modeling features for unlimited BPMN/DMN models.\nRegister to access Camunda Console for 1 month trial period\nCamunda Console is available as a free account (trial): Installed Camunda Modeler (available as Opensource)\nSample Applications # Create a Microservices orchestration flow using the modeler like below: You can also use their CLI interface: zbctl create instance \u0026#34;order-process\u0026#34; Created a simple process to call a REST endpoint: More code examples are available here: https://github.com/camunda/camunda-bpm-examples Connect your local desktop setup with cloud as shown below for deployment: Screenshots # Optmize dashboard listing all created dashboards (ready-made available) Optimize Heatmap displays the incidence of higher outliers based on duration. Calculated by z-score. Useful Links: # Downloads: https://camunda.com/download/modeler/ https://camunda.com/blog/2021/10/start-and-step-through-a-process-with-rest-feat-swaggerui/ https://github.com/camunda https://docs.camunda.io/ https://docs.camunda.io/docs/guides/orchestrate-microservices/ "},{"id":14,"href":"/docs/devops/argocd/","title":"Argo CD","section":"DevOps/GitOps","content":" Argo CD # Argo CD is a declarative, GitOps continuous delivery tool for Kubernetes. It is part of Argo project, which has 30+ repositories and has projects as Argo Workflow, Argo Rollouts, and Argo Events.\nKey objectives # Application definitions, configurations, and environments should be declarative and version controlled. Application deployment and lifecycle management should be automated, auditable, and easy to understand. Links # Doccumentation Project Home Page GitHub Repo Reviewed in Mar 2022\nArgo CD Architecture # (Source: https://argo-cd.readthedocs.io/en/stable/)\nLocal Setup # Instructions available at the Getting Started guide helps to setup locally Step 1: Setup Kubernetes (Minikube, Rancher Desktop, Docker Desktop) Step 2: Create namespace \u0026amp; deploy to Kubernetes cluster: kubectl create namespace argocd kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Step 3: Installed CLI on Mac: brew install argocd Step 4: Added the Load Balancer and Port Forwarded for local access: kubectl patch svc argocd-server -n argocd -p \u0026#39;{\u0026#34;spec\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;LoadBalancer\u0026#34;}}\u0026#39; kubectl port-forward --address 0.0.0.0 svc/argocd-server -n argocd 8082:443 Don\u0026rsquo;t forget to add 0.0.0.0 address otherwise you can\u0026rsquo;t use IP Address (alternative is to use localhost)\nFirst Deployment # Example repository is available here: https://github.com/argoproj/argocd-example-apps User Id is admin (automatically setup initially) Password needs to be retrieved using: kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\u0026#34;{.data.password}\u0026#34; | base64 -d; echo Login to Console: https://localhost:8082 Login to CLI: argocd login localhost:8082 Deploy the app using CLI or use Dashboard: argocd app create guestbook --repo https://github.com/argoproj/argocd-example-apps.git --path guestbook --dest-server https://kubernetes.default.svc --dest-namespace default` Local Setup Snapshot # Key Observations # Intuit started ArgoCD and currently using to run its production workload of 100+ nodes cluster Argo was accepted to CNCF on Apr, 2020 and is at the Incubating project maturity level. Companies using Red Hat, Google, Ticketmaster, WordPress, NVIDIA, Tesla, Adobe ArgoCD provides GUI in comparison to Flux CD Supports raw Kubernetes manifests (YAML), Kustomize, and Helm. Argo CD has RBAC-based access control and it is independent of Kubernetes. Deployment: Argo CD can be configured in a “pull” (instance of Argo CD on each cluster) or “push” (one Argo CD instance connecting to many clusters). Compare ArgoCD vs. Flux # Read the article: https://thenewstack.io/gitops-on-kubernetes-deciding-between-argo-cd-and-flux "},{"id":15,"href":"/docs/platforms/backstage/","title":"Backstage","section":"Platforms","content":" Backstage # Backstage, developed by Spotify, is an open platform for building developer portals. You can centralize software catalog, microservices catalog, infrastructure tooling, documentation (ADRs), etc. in a single place. Click here to read the Spotify blog post providing overview. GitHub: https://github.com/backstage/backstage Demo\nOnline Demo: https://demo.backstage.io Local setup # Guide: https://backstage.io/docs/getting-started/ Crate a new backstage app using npx: npx @backstage/create-app yarn dev Local URL: http://localhost:3000 Setting up database: brew install postgresql brew services restart postgresql@14 or /usr/local/opt/postgresql@14/bin/postgres -D /usr/local/var/postgresql@14 #Login in local postgresql psql postgres -- to check postgres connection CREATE USER backstage WITH PASSWORD \u0026#39;secret\u0026#39; CREATEDB LOGIN Update configuration to use database: yarn add --cwd packages/backend pg Update app-config.yaml: database: #client: better-sqlite3 #connection: \u0026#39;:memory:\u0026#39; client: pg connection: host: \u0026#39;localhost\u0026#39; port: 5432 user: backstage password: **** Key Plugins # A plugin might needs to be installed on frontend as well as backend (NodeJS) applications.\nTechDocs SonarQube Jenkins Google Lighthouse ADR ADR Backend Cost Insights Sometimes plugin installation does not work. Use below: yarn install --check-files yarn tsc Configuring Sonar # Install Sonar Frontend Plugin Install Sonar Backend Plugin Generate User Token from Sonar (Login Required): My Account -\u0026gt; Security (snapshot below) Sonar API works without any password if you have the user token Ensure that plugin got installed correctly by running following - it should be in : yarn tsc find . -name \u0026#39;*sonar*\u0026#39; You should see following files: ./node_modules/@backstage/plugin-sonarqube ./node_modules/@backstage/plugin-sonarqube-backend ./packages/backend/src/plugins/sonarqube.ts Get the Sonar Project Key from Dashboard -\u0026gt; Project -\u0026gt; Project Information (see below) Update sonar settings in app-config.yaml: sonarqube: instances: - name: default baseUrl: http://localhost:9000 # API Key is the user token generated from Sonar apiKey: ************ Put the configuration in your project catalog-info.yaml: annotations: backstage.io/techdocs-ref: dir:. jenkins.io/job-full-name: \u0026#34;devlocal:cloudcost-inspector\u0026#34; sonarqube.org/project-key: \u0026#34;com.ps.cloudcostinspector:CloudCostInspector\u0026#34; Configuring TechDocs # TechDocs is Spotify’s homegrown docs-like-code solution built directly into Backstage. It uses MkDocs as the underlying engine as static site generator. You can generate documents outside the Backstage (separate CI/CD) Detailed instructions are here: TechDocs configuration options brew install graphviz brew install plantuml pip install mkdocs pip install mkdocs-techdocs-core==1.1.7 techdocs-cli serve --no-docker --for testing docs Configure Lighthouse # You need to install Lighthouse Plugin Lighthouse Audit Service is a separate Microservice, created by Spotify. This needs to run separately as a container. See instructions here. You need to create a DB Schema in PostgreSQL (see below diagram), and Run Lighthouse audit service Backstage URL: http://localhost:3000/lighthouse the sidebar is managed inside packages/app/src/components/Root/Root.tsx Configuring Jenkins # "},{"id":16,"href":"/docs/devops/fluxcd/","title":"Flux CD","section":"DevOps/GitOps","content":" Flux CD # Flux is a set of continuous and progressive delivery solutions for Kubernetes that are open and extensible. Flux is a tool for keeping Kubernetes clusters in sync with sources of configuration (like Git repositories), and automating updates to configuration when there is new code to deploy. Flux v2 is constructed with the GitOps Toolkit, a set of composable APIs and specialized tools for building Continuous Delivery on top of Kubernetes. Flux Links\nGitHub: https://github.com/fluxcd/flux2 Local setup # Guide: https://fluxcd.io/docs/get-started/ Install using brew brew install fluxcd/tap/fluxbrew install fluxcd/tap/flux export GITHUB_TOKEN=\u0026lt;PUT_TOKEN\u0026gt; export GITHUB_USER=ankurkumarz flux check --pre Enable Flux for a project repository flux bootstrap github \\ --owner=$GITHUB_USER\\ --repository=fleet-infra \\ --branch=main \\ --path=./clusters/my-cluster \\ --personal "},{"id":17,"href":"/docs/platforms/hasura/","title":"Hasura","section":"Platforms","content":" Context # Hasura is a Opensource GraphQL engine and a developer platform, which can generate GraphQL API directly from the database. It has built-in support for authorization and caching acclerating developer\u0026rsquo;s productivity. Hasura offers a managed service (public cloud hosted) for easy access. Hasura has raised $100M Series C funding in February 2022 (source)\nSetup # Cloud-managed service by Hasura: https://cloud.hasura.io/\nIt can be deployed to Azure, Google Cloud, AWS, Render and others.\nIt provides creating a database (Postgres) in partnership with Neon.\nYou can connect below databases: It uses Docusaurus for documentation.\nPros/Cons # Pros:\nHelps to acclerate development considering the easy way to create GraphQL based API from the database Provides out-of-the-box caching, monitoring, custom queries (Actions), event-based processing (Event Trigger), and more. Cons/Cautions:\nFor financial services, connectivity with Hasura-managed Cloud will be a concern. Alternatively, it can hosted by the enterprise. Scalability and support in case of production issue (considering the scale of the organization) Good for simple use-cases exposing data as CRUD, need to explore more on complex business-logic components. Hasura Console # Interface to publish API: Other Competitors # Apollo GraphOS WunderGraph Firebase (Google Cloud) AWS AppSync Postgraphile Supabase Architecture Patterns # Event Triggers (captures events (insert, update, delete) on a specified table and then reliably calls a HTTP webhook to run some custom business logic) Custom queries or mutations: "},{"id":18,"href":"/docs/devops/jenkins/","title":"Jenkins","section":"DevOps/GitOps","content":" Jenkins # Jenkins is the one of the leading opensource CI/CD (or automation) tool prevalent in the industry. Most widely used and the massieve support available from community. Improvement Areas # While there are many advantages of Jenkins, these are well-known limitation or improvement areas:\nUser experience though there are improvements using plugins like Blue Ocean Cloud-native nature in the age of alternatives like Tekton. Local Installation # Installation instructions available here. brew install jenkins-lts brew services start jenkins-lts Dashboard URL (Local): http://localhost:8080/ Password on MacOS in file: /Users/ankkumar/.jenkins/secrets/initialAdminPassword Generated a GitHub token and connected it with Jenkins Key Plugins # Apart from default plugins, these are relevant while it is contextual:\nBlue Ocean - for redefined user experience Docker - for docker integration Audit Log - for details of who, when and what actions Logstask / Kafka Logs / Syslog - for sending logs to centralized logging server Terraform - wrapper for Terraform SonarQube - for code quality. Configure as per instructions here. Manage Jenkins -\u0026gt; Configure system Snyk Security - Installation Instructions, Configuration Instructions "},{"id":19,"href":"/docs/genai/langgraph/","title":"LangGraph","section":"Gen AI","content":" LangGraph # LangGraph is a framework by LangChain for building agentic and multi-agent applications Intro Course LangGraph Studio is available as an IDE for Desktop and Cloud. LangGraph Platform is proprietary software, separate from LangGraph library. Documentation Deployment # ‍Cloud SaaS (Paid): Fully managed and hosted as part of LangSmith Self-hosted Lite (free): Up to 1M nodes, limited version of LangGraph Platform. ‍Bring Your Own Cloud (Paid): Deploy LangGraph Platform within your VPC, provisioned and run as a service. Keep data in your environment while outsourcing the management of the service. Self-hosted Enterprise (Paid): Deploy LangGraph entirely on your own infrastructure with LangGraph Platform. Self-hosted (Free) as a Python library with a server like FastAPI. Code Examples # Framework with Toolkit-like approach for Self-hosting LangGraph Tutorials Local Setup # cd sandbox/genai/langgraph/langchain-academy "},{"id":20,"href":"/docs/genai/langsmith/","title":"LangSmith","section":"Gen AI","content":" LangSmith # Getting Started / Setup # "},{"id":21,"href":"/docs/lowcodeplatforms/","title":"Low-code Development Platform (LCDP)","section":"Docs","content":" Low-code Platforms (LCAP) # Gartner Definition of Low-code # A low-code application platform (LCAP) is used to rapidly develop and deploy custom applications by abstracting and minimizing hand coding. At a minimum, an LCAP must include low-code capabilities (such as a model-driven or graphical programming approach with scripting) to develop a complete application consisting of user interfaces, business logic, workflow and data services. Low-code is the evolution of RAD to cloud and SaaS models. Note that Gartner defines a no-code application platform as an LCAP that only requires text entry for formulae or simple expressions. The LCAP market, therefore, includes no-code platforms. Click here to read more. Low-code platforms # Vendor Solution Solution Description Industry Recognitions Appian Appian Mendix Mendix "},{"id":22,"href":"/docs/serverless/momento/","title":"Momento","section":"Serverless","content":" Momento # Context # Momento is a Serverless Data Platform focused on providing minimal effort to create Cache and Pub/Sub (Topics) services in Cloud. It is also working on providing Vector Index (not database) as a service for Gen AI. You can choose the Cloud Service Provider (AWS \u0026amp; GCP supported). When you publish an item to a topic, if it does not exist yet, the topic is created. As Topics utilizes Momento Cache, there is no scaling configuration either. It provides SDK in more than 10+ languages. It provides Free tier (first 5 GB/month, 100 TPS Max). Momento Console # Console Link Registered with GitHub credentials Create a Cache # List Cache # Publish/Subscribe to Topic with Cache # Installation Local for CLI # brew tap momentohq/tap brew install momento-cli momento configure --quick momento cache set key value --ttl 100 --cache example-cache momento cache get key --cache example-cache Documentation # CLI Documentation Developer Documentation "},{"id":23,"href":"/docs/genai/nemo-guardrails/","title":"NeMo Guardrails","section":"Gen AI","content":" NeMo-Guardrails # Nemo guardrails provides\nNeMo Guardrails is an open-source toolkit, built by NVIDIA, for easily adding programmable guardrails to LLM-based conversational systems.\nIt is built using LangChain and supports integrated guardrails for LangChain applications.\nIt offers Chat, Server, and Library interface to experiment with Guardrails.\nClick here for starter kit.\nGetting Started / Setup # Install in your Python environment: pip install nemoguardrails Chat Experience # To launch chat experience using CLI: nemoguardrails chat --config /Users/ankkumar/sandbox/genai/nemo/NeMo-Guardrails//docs/getting_started/4_input_rails/config To launch chat experience using Web UI: nemoguardrails server --config /Users/ankkumar/sandbox/genai/nemo/NeMo-Guardrails//docs/getting_started/4_input_rails/config "},{"id":24,"href":"/docs/serverless/openfaas/","title":"OpenFaaS","section":"Serverless","content":" OpenFaaS # Context # OpenFaaS makes it easy for developers to deploy event-driven functions and microservices to Kubernetes without repetitive, boiler-plate coding. OpenFaaS Pro is a commercially licensed distribution of OpenFaaS with additional features, configurations and commercial support from the founders. Supports Opensource framework avoiding vendor lock-in Write functions in any language and package them in Docker/OCI-format containers Supports PLONK (Prometheus, Linkerd/Linux, OpenFaas, NATS, K8S) technology stack: Conceptual flow: Local setup # Installed Multipass on MAC (instead of using Docker, it is lightweight). Test Multipass using: multipass launch --name foo multipass exec foo -- lsb_release -a multipass list Create a Cloud Config File and replace the SSH Key: curl -sSLO https://raw.githubusercontent.com/openfaas/faasd/master/cloud-config.txt cat ~/.ssh/id_rsa.pub Replace ssh_authorized_keys with above SSH Key Launch faasd with Multipass: cat cloud-config.txt | multipass launch --name faasd --cloud-init - multipass info faasd Launch the console: export IP=$(multipass info faasd --format json| jq -r \u0026#39;.info.faasd.ipv4[0]\u0026#39;) ssh ubuntu@$IP \u0026#34;sudo cat /var/lib/faasd/secrets/basic-auth-password\u0026#34; \u0026gt; basic-auth-password export OPENFAAS_URL=http://$IP:8080 Username is admin and password is in the basic-auth-password file Deploy a sample serverless function: cat basic-auth-password | faas-cli login -s faas-cli store deploy figlet --env write_timeout=1s echo \u0026#34;faasd\u0026#34; | faas-cli invoke figlet "},{"id":25,"href":"/docs/platforms/salesforce/","title":"Salesforce","section":"Platforms","content":" Local Setup # CLI Installation # See instructions here: npm install sfdx-cli --global npm install @salesforce/cli --global sfdx --version sfdx plugins --core sfdx commands VS Code Extension # Extension name is \u0026ldquo;Salesforce Extension Pack\u0026rdquo; -In Visual Studio Code, open the Command Palette by pressing Ctrl+Shift+P (Windows) or Cmd+Shift+P (macOS/Linux). Enter SFDX to filter for commands provided by the Salesforce extensions. "},{"id":26,"href":"/docs/devops/sonarqube/","title":"Sonarqube","section":"DevOps/GitOps","content":" Sonarqube # SonarQube, is the most widely used automatic code review tool that systematically helps deliver Clean Code. It supports Kubernetes and Docker for installation. It provides DevOps platform integration with GitHub, GitLab, Bitbucket, Azure DevOps, and others. Local Installation # Installation instructions: brew install sonarqube brew services start sonarqube Dashboard URL (Local): http://localhost:9000/ - default userid password (admin/admin). For enterprise installation, needs to ensure scalability as it has 3 components - Server (Web server, Search server, Compute engine), Database, and Scanner (one or more based on the projects). Read more about installation in an enterprise. "},{"id":27,"href":"/docs/devops/terraform/","title":"Terraform","section":"DevOps/GitOps","content":" Terraform # Terraform is HashiCorp\u0026rsquo;s infrastructure as code tool. It lets you define resources and infrastructure in human-readable, declarative configuration files, and manages your infrastructure\u0026rsquo;s lifecycle. It supports multiple cloud providers - AWS, Azure, GCP, Oracle Cloud, and others (1K+ providers). Click here to access the tutorial. Notes # Terraform Registry for providers, modules and libraries. Terraform Language- sing a JSON-like configuration language called HCL (HashiCorp Configuration Language), its backend is written in Golang. Use VS Code extension for syntax highlights When you applied your configuration, Terraform writes data into a state file called terraform.tfstate. It uses JSON format to store data. State can be stored in Terraform Cloud or Cloud-providers like AWS S3 or Google Cloud Storage. Click here to read more about remote state. Local Installation # brew tap hashicorp/tap brew update brew install hashicorp/tap/terraform terraform -help terraform -install-autocomplete terraform fmt terraform validate Run Terraform to create a Docker container # Create a file \u0026ldquo;main.tf\u0026rdquo; terraform { required_providers { docker = { source = \u0026#34;kreuzwerker/docker\u0026#34; version = \u0026#34;~\u0026gt; 2.13.0\u0026#34; } } } provider \u0026#34;docker\u0026#34; {} resource \u0026#34;docker_image\u0026#34; \u0026#34;nginx\u0026#34; { name = \u0026#34;nginx:latest\u0026#34; keep_locally = false } resource \u0026#34;docker_container\u0026#34; \u0026#34;nginx\u0026#34; { image = docker_image.nginx.latest name = \u0026#34;tutorial\u0026#34; ports { internal = 80 external = 8000 } } Run the following command: terraform init terraform plan terraform apply terraform destroy "},{"id":28,"href":"/docs/genai/wand/","title":"Weights \u0026 Biases","section":"Gen AI","content":" RAG with Weights \u0026amp; Biases # Using Weave, a lightweight toolkit by W\u0026amp;B for tracking and evaluating LLM application Using Weaviate as Vector Database. Registered for Weavite Cloud using Gmail. Sandbox clusters. Sandbox clusters are small, free clusters for 14 days. Recommended Course: RAG in Prod Sample RAG Bot Course Code Local Setup # cd sandbox/genai/wand/edu/rag-advanced/notebooks "},{"id":29,"href":"/docs/genai/zenml/","title":"ZenML","section":"Gen AI","content":" Context # ZenML is an extensible, open-source MLOps framework for creating portable, production-ready MLOps pipelines. GitHub Repo Production Guide ZenML Comparison with Airflow, ClearML, Kedro, Kubeflow, Metaflow, Dagster, etc. Fine-tuning open source LLMs using MLOps pipelines with PEFT Quickstart Guide # For Opensource: cd ../sandbox/genai/zenml pyenv global 3.11 python -m venv .venv source .venv/bin/activate pip install \u0026#34;zenml[server]\u0026#34; notebook export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES zenml login --local zenml go -- Quickly explore ZenML with this walk-through Running a Pipeline # Architecture # "}]